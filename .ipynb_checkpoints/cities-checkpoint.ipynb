{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import webbrowser\n",
    "import csv\n",
    "from IPython.display import clear_output as clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "# ./202006/t20200630_2149462.htm\n",
    "# http://wjw.yinchuan.gov.cn/wsjsdt/202006/t20200630_2149462.htm\n",
    "path = \"test.csv\"\n",
    "base = \"http://wjw.yinchuan.gov.cn/wsjsdt\"\n",
    "\n",
    "def getCon(url):\n",
    "    con = \"\"\n",
    "    html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    soup = bs(html, features='lxml')\n",
    "    findDiv = soup.find('div', attrs={'class':'con'})\n",
    "    if findDiv:\n",
    "        findP = findDiv.find_all('p')\n",
    "        for eachP in findP:\n",
    "            con += eachP.get_text()\n",
    "    return con\n",
    "\n",
    "def getCsv(link):\n",
    "    html = urllib.request.urlopen(link).read().decode('utf-8')\n",
    "    soup = bs(html, features='lxml')\n",
    "    all_li = soup.find('ul', attrs={'class':'list2'}).find_all('li') \n",
    "    for li in all_li:\n",
    "        with open(path, 'a+') as f:\n",
    "            csv_write = csv.writer(f)\n",
    "            dataRow = []\n",
    "            spanTag = li.find_all('span')\n",
    "            dataRow.append(spanTag[0].get_text())\n",
    "            aTag = li.find_all('a')\n",
    "            dataRow.append(aTag[0].get_text())\n",
    "            if \"http\" in aTag[0]['href']:\n",
    "                url = aTag[0]['href']\n",
    "            else:\n",
    "                url = base+str(aTag[0]['href'])[1:]\n",
    "            dataRow.append(url)\n",
    "            dataRow.append(getCon(url))\n",
    "            csv_write.writerow(dataRow)\n",
    "            f.close()\n",
    "\n",
    "pageUrlBase = \"http://wjw.yinchuan.gov.cn/wsjsdt/index\"\n",
    "pageUrlBaseTail = \".htm\"\n",
    "for i in range(10):\n",
    "    pageMid = \"\"\n",
    "    print(i)\n",
    "    if i == 0:\n",
    "        getCsv(pageUrlBase+pageUrlBaseTail)\n",
    "    else:\n",
    "        getCsv(pageUrlBase+'_'+str(i)+pageUrlBaseTail)\n",
    "    \n",
    "print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# http://wsjkw.weifang.gov.cn/ZWDT/TZGG/default_3.htm\n",
    "# http://wsjkw.weifang.gov.cn/ZWDT/TZGG/default.htm\n",
    "# http://wsjkw.weifang.gov.cn/YWZT/ZYJK/202006/t20200623_5630010.htm\n",
    "path = \"weifang.csv\"\n",
    "base = \"http://wsjkw.weifang.gov.cn\"\n",
    "\n",
    "def getCon(url):\n",
    "    con = \"\"\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'}\n",
    "    html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    soup = bs(html, features='lxml')\n",
    "    findDiv = soup.find('div', attrs={'class':'TRS_PreAppend'})\n",
    "    if findDiv == None:\n",
    "        findDiv = soup.find_all('div', attrs={'class':'TRS_Editor'})\n",
    "        if findDiv != None:\n",
    "            con += findDiv[1].get_text()\n",
    "    else:\n",
    "        con += findDiv.get_text()\n",
    "    return con\n",
    "\n",
    "#     if findDiv:\n",
    "#         findP = findDiv.find_all('p')\n",
    "#         for eachP in findP:\n",
    "#             con += eachP.get_text()\n",
    "            \n",
    "def getCsv(link):\n",
    "    html = urllib.request.urlopen(link).read().decode('utf-8')\n",
    "    soup = bs(html, features='lxml')\n",
    "    all_td = soup.find_all('td', attrs={'class':'link02'})\n",
    "    i,length = 0,len(all_td)\n",
    "    while i < length :\n",
    "        img = all_td[i].find('img')\n",
    "        if img != None:\n",
    "            i += 1\n",
    "            continue\n",
    "        with open(path, 'a+') as f:\n",
    "            csv_write = csv.writer(f)\n",
    "            dataRow = []\n",
    "            aTag = all_td[i].find_all('a')\n",
    "            if aTag != []:\n",
    "                dataRow.append(aTag[0].get_text())\n",
    "                if aTag[0]['href'][1] == '.':\n",
    "                    url = base + str(aTag[0]['href'][5:])\n",
    "                else:\n",
    "                    url = base + \"/ZWDT/TZGG\" + str(aTag[0]['href'])[1:]\n",
    "                dataRow.append(url)\n",
    "                dataRow.insert(0,all_td[i+1].get_text())\n",
    "                dataRow.append(getCon(url))\n",
    "                csv_write.writerow(dataRow)\n",
    "            f.close()\n",
    "        i += 2\n",
    "# getCsv(\"http://wsjkw.weifang.gov.cn/ZWDT/TZGG/default.htm\")      \n",
    "pageUrlBase = \"http://wsjkw.weifang.gov.cn/ZWDT/TZGG/default\"\n",
    "pageUrlBaseTail = \".htm\"\n",
    "for i in range(4):\n",
    "    pageMid = \"\"\n",
    "    if i == 0:\n",
    "        url = pageUrlBase+pageUrlBaseTail\n",
    "    else:\n",
    "        url = pageUrlBase+'_'+str(i)+pageUrlBaseTail\n",
    "    getCsv(url)\n",
    "    print(i)\n",
    "print(\"finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pritn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-ac422d8f5ea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#     print(i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mgetCsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://wsjkw.rizhao.gov.cn/col/col119693/index.html?uid=121744&pageNum=0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-ac422d8f5ea3>\u001b[0m in \u001b[0;36mgetCsv\u001b[0;34m(link)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'121744'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mall_rtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'record'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mpritn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_rtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m#     for rtag in all_rtag:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#         atext = rtag.get_text()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pritn' is not defined"
     ]
    }
   ],
   "source": [
    "# ./202006/t20200630_2149462.htm\n",
    "# http://wjw.yinchuan.gov.cn/wsjsdt/202006/t20200630_2149462.htm\n",
    "from bs4 import CData\n",
    "import html.parser\n",
    "import re\n",
    "\n",
    "path = \"日照.csv\"\n",
    "\n",
    "def getCon(url):\n",
    "    con = \"\"\n",
    "    html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    soup = bs(html, features='lxml')\n",
    "    findDiv = soup.find('div', attrs={'id':'js_content'})\n",
    "    if findDiv == None:\n",
    "        findDiv = soup.find_all('div', attrs={'class':'content'})\n",
    "        if findDiv != None:\n",
    "            con += findDiv[0].get_text()\n",
    "    else:\n",
    "        con += findDiv.get_text()\n",
    "    return con\n",
    "\n",
    "def getCsv(link):\n",
    "    html = urllib.request.urlopen(link).read().decode(\"utf-8\")\n",
    "#     soup = bs(html, ['lxml', 'xml'])\n",
    "    soup = bs(html, 'xml')\n",
    "    div = soup.find('div', attrs={'id':'121744'})\n",
    "    all_rtag = div.find_all('record')\n",
    "    print(all_rtag)\n",
    "#     for rtag in all_rtag:\n",
    "#         atext = rtag.get_text()\n",
    "#         with open(path, 'a+') as f:\n",
    "#             csv_write = csv.writer(f)\n",
    "#             dataRow = []\n",
    "#             url = str(re.findall(r'<a href=\"(.*?)\"', atext)[0])\n",
    "#             title = str(re.findall(r'<span class=\"cn\">(.*?)</span>', atext)[0])\n",
    "#             time = str(re.findall(r'<span class=\"time\">(.*?)</span></a>',atext)[0])\n",
    "#             dataRow.append(time)\n",
    "#             dataRow.append(title)\n",
    "#             dataRow.append(url)\n",
    "#             dataRow.append(getCon(url))\n",
    "#             csv_write.writerow(dataRow)\n",
    "#             f.close()\n",
    "\n",
    "pageUrlBase = \"http://wsjkw.rizhao.gov.cn/col/col119693/index.html?uid=121744&pageNum=\"\n",
    "\n",
    "# for i in range(1):\n",
    "#     pageMid = \"\"\n",
    "# #     if i == 0:c\n",
    "# #         getCsv(pageUrlBase+pageUrlBaseTail)\n",
    "#     getCsv(pageUrlBase+str(i))\n",
    "#     print(i)\n",
    "\n",
    "getCsv(\"http://wsjkw.rizhao.gov.cn/col/col119693/index.html?uid=121744&pageNum=0\")\n",
    "    \n",
    "print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "path = \"汕尾.csv\"\n",
    "# http://www.shanwei.gov.cn/swwjj/yaowen/tzgg/index.html\n",
    "# http://www.shanwei.gov.cn/swwjj/yaowen/tzgg/index_12.html\n",
    "\n",
    "def getCon(url):\n",
    "    con = \"\"\n",
    "    html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    soup = bs(html, features='lxml')\n",
    "    soup.find('div', id=\"glgj\").decompose()\n",
    "    soup.find('div', id=\"xgfj\").decompose()\n",
    "    findDiv = soup.find('div', attrs={'class':'zwgk_comr3 line'})\n",
    "#     if findDiv == None:\n",
    "#         findDiv = soup.find_all('div', attrs={'class':'content'})\n",
    "#         if findDiv != None:\n",
    "#             con += findDiv[0].get_text()\n",
    "#     else: \n",
    "    con += findDiv.get_text()\n",
    "    return con\n",
    "\n",
    "\n",
    "def getCsv(url):\n",
    "    html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    soup = bs(html,'lxml')\n",
    "    allDiv = soup.find_all('div', attrs={'class':\"list_div mar-top2\"})\n",
    "    for div in allDiv:\n",
    "        dataRow = []\n",
    "        titlePart = div.find('div', attrs={'class':\"list-right_title fon_1\"})\n",
    "        aTag = titlePart.find('a')\n",
    "        link = aTag['href']\n",
    "        title = aTag.get_text()\n",
    "        time = div.find('table').find('td').get_text()\n",
    "        with open(path, 'a+') as f:\n",
    "            csv_writer = csv.writer(f)\n",
    "            dataRow.append(time)\n",
    "            dataRow.append(title)\n",
    "            dataRow.append(link)\n",
    "            dataRow.append(getCon(link))\n",
    "            csv_writer.writerow(dataRow)\n",
    "            f.close()\n",
    "\n",
    "base = \"http://www.shanwei.gov.cn/swwjj/yaowen/tzgg/index\"\n",
    "for i in range(12):\n",
    "    if i == 0:\n",
    "        tail = \".html\"\n",
    "    else:\n",
    "        tail = \"_\"+str(i+1)+\".html\"\n",
    "    url = base+tail\n",
    "    getCsv(base+tail)\n",
    "    print(i)\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "path = \"东营.csv\"\n",
    "# http://www.dongying.gov.cn/col/col119756/index.html?uid=127043&pageNum=1\n",
    "# http://www.dongying.gov.cn/col/col119756/index.html?uid=127043&pageNum=38\n",
    "\n",
    "def getCon(url):\n",
    "    con = \"\"\n",
    "    html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    soup = bs(html, features='lxml')\n",
    "    soup.find('div', id=\"glgj\").decompose()\n",
    "    soup.find('div', id=\"xgfj\").decompose()\n",
    "    findDiv = soup.find('div', attrs={'class':'zwgk_comr3 line'})\n",
    "    con += findDiv.get_text()\n",
    "    return con\n",
    "\n",
    "titleList = []\n",
    "def getCsv(url):\n",
    "    html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    soup = bs(html,'xml')\n",
    "    div = soup.find('div', attrs={'id':\"127043\"})\n",
    "#     print(div)\n",
    "    recordSet = div.find('recordset')\n",
    "    all_rtag = recordSet.find_all('record')\n",
    "    for rtag in all_rtag:\n",
    "        atext = rtag.get_text()\n",
    "        with open(path, 'a+') as f:\n",
    "            csv_write = csv.writer(f)\n",
    "            dataRow = []\n",
    "            url = str(re.findall(r'<a href=\"(.*?)\"', atext)[0])\n",
    "            title = str(re.findall(r'title=\"(.*?)\"', atext)[0])\n",
    "            if title in titleList:\n",
    "                f.close()\n",
    "                continue\n",
    "            time = str(re.findall(r'<span class=\"bt-data-time\">(.*?)</span>',atext)[0])\n",
    "            dataRow.append(time)\n",
    "            dataRow.append(title)\n",
    "            dataRow.append(url)\n",
    "#             dataRow.append(getCon(url))\n",
    "            csv_write.writerow(dataRow)\n",
    "            f.close()\n",
    "        \n",
    "base = \"http://www.dongying.gov.cn/col/col119756/index.html?uid=127043&pageNum=\"\n",
    "# for i in range(1,39):\n",
    "#     print(i)\n",
    "#     time.sleep(2)\n",
    "#     url = base+str(i)\n",
    "#     getCsv(url)\n",
    "\n",
    "getCsv(\"http://www.dongying.gov.cn/col/col119756/index.html?uid=127043&pageNum=4\")\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 临沂 \n",
    "path = \"临沂.csv\"\n",
    "paper_base = \"http://wsjsw.linyi.gov.cn/\"\n",
    "\n",
    "all_paper = []\n",
    "\n",
    "def getCon(url):\n",
    "    con = \"\"\n",
    "    html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    soup = bs(html, 'lxml')\n",
    "    div = soup.find('div', attrs={'class':'Article_Content'})\n",
    "    \n",
    "    try:\n",
    "        con += div.get_text()\n",
    "    except Exception:\n",
    "        print(html)\n",
    "        \n",
    "    return con\n",
    "\n",
    "def getCsv(url):\n",
    "    html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    soup = bs(html,'lxml')\n",
    "    ul = soup.find('ul', attrs={'class':\"Article_List\"})\n",
    "    all_li = ul.find_all('li')\n",
    "    for li in all_li:\n",
    "        spanTag = li.find_all('span')\n",
    "        aTag = li.find_all('a')\n",
    "        if aTag[0].get_text() in all_paper:\n",
    "            continue\n",
    "        all_paper.append(aTag[0].get_text())\n",
    "        if \"http\" in aTag[0]['href']:\n",
    "            url = aTag[0]['href']\n",
    "        else:\n",
    "            if aTag[0]['href'][0] == '.':\n",
    "                url = paper_base + str(aTag[0]['href'][2:])\n",
    "            else:\n",
    "                url = paper_base+str(aTag[0]['href'])\n",
    "        with open(path, 'a+') as f:\n",
    "            csv_write = csv.writer(f)\n",
    "            dataRow = []\n",
    "            dataRow.append(spanTag[0].get_text())\n",
    "            dataRow.append(aTag[0].get_text())\n",
    "            dataRow.append(url)\n",
    "            dataRow.append(getCon(url))\n",
    "            csv_write.writerow(dataRow)\n",
    "            f.close()\n",
    "            \n",
    "urlList = [\n",
    "    \"http://wsjsw.linyi.gov.cn/gsgg.htm\",\n",
    "    \"http://wsjsw.linyi.gov.cn/gsgg/28.htm\",\n",
    "    \"http://wsjsw.linyi.gov.cn/gsgg/27.htm\"\n",
    "]\n",
    "for l in urlList:\n",
    "    getCsv(l)\n",
    "\n",
    "# clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 佳木斯 \n",
    "path = \"佳木斯.csv\"\n",
    "titleList = []\n",
    "paperBase = \"http://www.jms.gov.cn/html/index/\"\n",
    "\n",
    "def getCon(url):\n",
    "    con = \"\"\n",
    "    try:\n",
    "        html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "        soup = bs(html,'lxml')\n",
    "        div = soup.find('div',attrs={'class':'jms_contentC'})\n",
    "        con += div.get_text()\n",
    "    except Exception:\n",
    "        print(url)\n",
    "    return con\n",
    "    \n",
    "def getCsv(url):\n",
    "    html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    soup = bs(html,'lxml')\n",
    "    ul = soup.find('ul',attrs={'class':\"jms_listUl\"})\n",
    "    allli = ul.find_all('li')\n",
    "    for li in allli:\n",
    "        atag = li.find('a')\n",
    "        url = paperBase+atag['href'][13:]\n",
    "        title = atag.get_text()\n",
    "        spantag = li.find('span')\n",
    "        date = spantag.get_text()\n",
    "        content = getCon(url)\n",
    "        with open(path, 'a+') as f:\n",
    "            csv_write = csv.writer(f)\n",
    "            rowData = []\n",
    "            rowData.append(date)\n",
    "            rowData.append(str(title))\n",
    "            rowData.append(url)\n",
    "            rowData.append(content)\n",
    "            csv_write.writerow(rowData)\n",
    "            f.close()\n",
    "            \n",
    "urlbase = \"http://www.jms.gov.cn/html/index/page/000100050009_\"\n",
    "for i in range(1,39):\n",
    "    getCsv(urlbase+str(i)+\".html\")\n",
    "    print(i)\n",
    "\n",
    "# getCsv(\"http://www.jms.gov.cn/html/index/page/000100050009_1.html\")\n",
    "clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常州\n",
    "# http://www.changzhou.gov.cn/ns_class/jjfkxxfy2/15\n",
    "# http://www.changzhou.gov.cn/ns_class/jjfkxxfy2/1\n",
    "path = \"常州.csv\"\n",
    "paperBase = \"http://www.changzhou.gov.cn\"\n",
    "\n",
    "def getCon(url):\n",
    "    con = \"\"\n",
    "    try:\n",
    "        html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "        soup = bs(html,'lxml')\n",
    "        div = soup.find('div',attrs={'class':'NewsContent'})\n",
    "        con += div.get_text()\n",
    "    except Exception:\n",
    "        print(url)\n",
    "    return con\n",
    "\n",
    "headers = {'User-Agent': 'User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'}\n",
    "\n",
    "def getCsv(url):\n",
    "    temp = urllib.request.Request(url,headers=headers)\n",
    "    html = urllib.request.urlopen(temp).read().decode('utf-8',\"ignore\")\n",
    "    soup = bs(html,'lxml')\n",
    "    allA = soup.find_all('a')\n",
    "    for atag in allA:\n",
    "        print(atag)\n",
    "        if 'ns_news' not in str(atag['href']):\n",
    "            continue\n",
    "        url = paperBase+atag['href']\n",
    "        title = atag.get_text()\n",
    "        date = atag.find_next_sibling().get_text()\n",
    "#         content = getCon(url)\n",
    "        with open(path, 'a+') as f:\n",
    "            csv_write = csv.writer(f)\n",
    "            rowData = []\n",
    "            rowData.append(date)\n",
    "            rowData.append(str(title))\n",
    "            rowData.append(url)\n",
    "            rowData.append(content)\n",
    "            csv_write.writerow(rowData)\n",
    "            f.close()\n",
    "\n",
    "getCsv(\"http://www.changzhou.gov.cn/ns_class/jjfkxxfy2/1\")\n",
    "# for i in range(1,16):\n",
    "#     getCsv(\"http://www.changzhou.gov.cn/ns_class/jjfkxxfy2/\"+str(i))\n",
    "clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bbbcea9a1ffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mgetCsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://www.fy.gov.cn/special/content/?_id=5e4533807f8b9a2d118b4574&page=1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://www.fy.gov.cn/special/content/?_id=5e4533807f8b9a2d118b4574&page=\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# for i in range(1,5):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-bbbcea9a1ffd>\u001b[0m in \u001b[0;36mgetCsv\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetCsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mallDiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"listright-box\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crawler/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crawler/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crawler/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crawler/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crawler/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crawler/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "# 阜阳 \n",
    "path = \"阜阳.csv\"\n",
    "\n",
    "paperBase = \"http://www.fy.gov.cn\"\n",
    "headers = {'User-Agent': 'User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Safari/537.36'}\n",
    "\n",
    "def getCsv(url):\n",
    "    temp = urllib.request.Request(url)\n",
    "    html = urllib.request.urlopen(temp).decode('utf-8')\n",
    "    soup = bs(html,'lxml')\n",
    "    allDiv = soup.find('div', attrs={'class':\"listright-box\"})\n",
    "    allli = allDiv.find('ul').find('li')\n",
    "    for li in allli:\n",
    "        spanTag = li.find_all('span')\n",
    "        aTag = li.find_all('a')\n",
    "        url = paperBase+str(aTag[0]['href'])\n",
    "        with open(path, 'a+') as f:\n",
    "            csv_write = csv.writer(f)\n",
    "            dataRow = []\n",
    "            dataRow.append(spanTag[0].get_text())\n",
    "            dataRow.append(aTag[0].get_text())\n",
    "            dataRow.append(url)\n",
    "#             dataRow.append(getCon(url))\n",
    "            csv_write.writerow(dataRow)\n",
    "            f.close()\n",
    "\n",
    "getCsv(\"http://www.fy.gov.cn/special/content/?_id=5e4533807f8b9a2d118b4574&page=1\")\n",
    "base = \"http://www.fy.gov.cn/special/content/?_id=5e4533807f8b9a2d118b4574&page=\"\n",
    "# for i in range(1,5):\n",
    "#     getCsv(base+str(i))\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 扬州 cont mt10\n",
    " \n",
    "path = \"扬州1.csv\"\n",
    "\n",
    "paperBase = \"http://wjw.yangzhou.gov.cn\"\n",
    "headers = {'User-Agent': 'User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'}\n",
    "\n",
    "def getCon(url):\n",
    "    con = \"\"\n",
    "    temp = urllib.request.Request(url,headers=headers)\n",
    "    html = urllib.request.urlopen(temp).read().decode('utf-8')\n",
    "    soup = bs(html,'lxml')\n",
    "    soup.find('dl', id=\"CMSPRO_APPENDIXS\").decompose()\n",
    "    try:\n",
    "        div = soup.find('div',attrs={'class':'content'})\n",
    "        con += div.get_text()\n",
    "    except Exception:\n",
    "        print(url)\n",
    "    return con\n",
    "        \n",
    "\n",
    "def getCsv(url):\n",
    "    print(url)\n",
    "    temp = urllib.request.Request(url,headers=headers)\n",
    "    try:\n",
    "        html = urllib.request.urlopen(temp).read().decode('utf-8')\n",
    "    except:\n",
    "        pass\n",
    "    soup = bs(html,'lxml')\n",
    "    allDiv = soup.find('div', attrs={'class':\"cont mt10\"})\n",
    "    allli = allDiv.find('ul').find_all('li')\n",
    "    for li in allli:\n",
    "        spanTag = li.find_all('span')\n",
    "        aTag = li.find_all('a')\n",
    "        url = paperBase+str(aTag[0]['href'])\n",
    "        with open(path, 'a+') as f:\n",
    "            csv_write = csv.writer(f)\n",
    "            dataRow = []\n",
    "            dataRow.append(spanTag[0].get_text())\n",
    "            dataRow.append(aTag[0].get_text())\n",
    "            dataRow.append(url)\n",
    "            dataRow.append(getCon(url))\n",
    "            csv_write.writerow(dataRow)\n",
    "            f.close()\n",
    "\n",
    "getCsv(\"http://wjw.yangzhou.gov.cn/yzwshjh/tzgg/wjw_list.shtml\")\n",
    "\n",
    "base = \"http://wjw.yangzhou.gov.cn/yzwshjh/tzgg/wjw_list\"\n",
    "for i in range(2,17):\n",
    "    getCsv(base+\"_\"+str(i)+\".shtml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 厦门\n",
    "# http://hfpc.xm.gov.cn/xwzx/tzgg/index_10.htm\n",
    "# http://hfpc.xm.gov.cn/xwzx/tzgg/index.htm\n",
    "\n",
    "path = \"厦门.csv\"\n",
    "paperBase = \"http://hfpc.xm.gov.cn/xwzx/tzgg\"\n",
    "\n",
    "def getCon(url):\n",
    "    con = \"\"\n",
    "    try:\n",
    "        html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "        soup = bs(html,'lxml')\n",
    "        div = soup.find('div', attrs={'class':\"TRS_Editor\"})\n",
    "        con += div.get_text()\n",
    "    except:\n",
    "        print(url)\n",
    "    return con\n",
    "    \n",
    "\n",
    "def getCsv(url):\n",
    "    print(url)\n",
    "    temp = urllib.request.Request(url,headers=headers)\n",
    "    try:\n",
    "        html = urllib.request.urlopen(temp).read().decode('utf-8')\n",
    "    except:\n",
    "        pass\n",
    "    soup = bs(html,'lxml')\n",
    "    allDiv = soup.find('div', attrs={'class':\"gl_list\"})\n",
    "    allli = allDiv.find('ul').find_all('li')\n",
    "    for li in allli:\n",
    "        spanTag = li.find_all('span')\n",
    "        aTag = li.find_all('a')\n",
    "        if \"http\" in aTag[0]['href']:\n",
    "            curl = str(aTag[0]['href'])\n",
    "        else:\n",
    "            curl = paperBase+str(aTag[0]['href'])[1:]\n",
    "        with open(path, 'a+') as f:\n",
    "            csv_write = csv.writer(f)\n",
    "            dataRow = []\n",
    "            dataRow.append(spanTag[0].get_text())\n",
    "            dataRow.append(aTag[0].get_text())\n",
    "            dataRow.append(curl)\n",
    "            dataRow.append(getCon(curl))\n",
    "            csv_write.writerow(dataRow)\n",
    "            f.close()\n",
    "\n",
    "getCsv(\"http://hfpc.xm.gov.cn/xwzx/tzgg/index.htm\")\n",
    "base = \"http://hfpc.xm.gov.cn/xwzx/tzgg/index\"\n",
    "for i in range(1,11):\n",
    "    print(i)\n",
    "    getCsv(base+\"_\"+str(i)+\".htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 汕头 \n",
    "# https://www.shantou.gov.cn/cnst/ztzl/xxgzbdgrfyyqfk/index_20.html\n",
    "# https://www.shantou.gov.cn/cnst/ztzl/xxgzbdgrfyyqfk/index.html\n",
    "\n",
    "path = \"汕头.csv\"\n",
    "\n",
    "def getCon(url):\n",
    "    con = \"\"\n",
    "    try:\n",
    "        html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "        soup = bs(html,'lxml')\n",
    "        div = soup.find('div', attrs={'class':\"zoomcon\"})\n",
    "        con += div.get_text()\n",
    "    except:\n",
    "        print(url)\n",
    "    return con\n",
    "\n",
    "def getCsv(url):\n",
    "    print(url)\n",
    "    temp = urllib.request.Request(url,headers=headers)\n",
    "    try:\n",
    "        html = urllib.request.urlopen(temp).read().decode('utf-8')\n",
    "    except:\n",
    "        pass\n",
    "    soup = bs(html,'lxml')\n",
    "    allDiv = soup.find('div', attrs={'class':\"wzlm_right\"})\n",
    "#     print(soup)\n",
    "#     print(allDiv)\n",
    "    allli = allDiv.find('ul').find_all('li')\n",
    "    for li in allli:\n",
    "        spanTag = li.find_all('span')\n",
    "        aTag = li.find_all('a')\n",
    "        if \"http\" in aTag[0]['href']:\n",
    "            curl = str(aTag[0]['href'])\n",
    "        else:\n",
    "            curl = paperBase+str(aTag[0]['href'])[1:]\n",
    "        with open(path, 'a+') as f:\n",
    "            csv_write = csv.writer(f)\n",
    "            dataRow = []\n",
    "            dataRow.append(spanTag[0].get_text())\n",
    "            dataRow.append(aTag[0].get_text())\n",
    "            dataRow.append(curl)\n",
    "            dataRow.append(getCon(curl))\n",
    "            csv_write.writerow(dataRow)\n",
    "            f.close()\n",
    "            \n",
    "getCsv(\"https://www.shantou.gov.cn/cnst/ztzl/xxgzbdgrfyyqfk/index.html\")\n",
    "for i in range(2,21):\n",
    "    getCsv(\"https://www.shantou.gov.cn/cnst/ztzl/xxgzbdgrfyyqfk/index\"+\"_\"+str(i)+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 504: Gateway Time-out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-bcfd20517624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mgetCsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://wjw.taian.gov.cn/col/col119732/index.html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;31m# for i in range(1,18):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m#     getCsv(\"http://wjw.taian.gov.cn/col/col119732/index.html?uid=286952&pageNum=\"+str(i))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-136-bcfd20517624>\u001b[0m in \u001b[0;36mgetCsv\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetCsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#     print(soup)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crawler/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crawler/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crawler/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crawler/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crawler/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crawler/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 504: Gateway Time-out"
     ]
    }
   ],
   "source": [
    "# 泰安 \n",
    "path = \"泰安.csv\"\n",
    "# http://wjw.taian.gov.cn/col/col119732/index.html?uid=286952&pageNum=1\n",
    "# http://wjw.taian.gov.cn/col/col119732/index.html?uid=286952&pageNum=17\n",
    "\n",
    "titleList = []\n",
    "headers = {'User-Agent': 'User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'}\n",
    "\n",
    "\n",
    "def getCon(url):\n",
    "    print(url)\n",
    "    con = \"\"\n",
    "    try:\n",
    "        html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "        soup = bs(html,'lxml')\n",
    "#         soup.find('div', id=\"main-fl-xyp\").decompose()\n",
    "#         soup.find('div', id=\"main-fl-tit\").decompose()\n",
    "#         soup.find('div', id=\"main-fl-riqi\").decompose()\n",
    "        td = soup.find('div',attrs={'class':\"main-fl bt-left\"})\n",
    "        con += td.get_text()\n",
    "    except Exception:\n",
    "        print(url)\n",
    "    return con\n",
    "\n",
    "def getCsv(url):\n",
    "    temp = urllib.request.Request(url,headers=headers)\n",
    "    html = urllib.request.urlopen(temp).read().decode('utf-8')\n",
    "    soup = bs(html,'xml')\n",
    "#     print(soup)\n",
    "    div = soup.find('div', attrs={'id':\"286952\"})\n",
    "    recordSet = div.find('recordset')\n",
    "    all_rtag = recordSet.find_all('record')\n",
    "    for rtag in all_rtag:\n",
    "        atext = rtag.get_text()\n",
    "        with open(path, 'a+') as f:\n",
    "            csv_write = csv.writer(f)\n",
    "            dataRow = []\n",
    "            url = str(re.findall(r'<a target=\"_blank\" href=\"(.*?)\"', atext)[0])\n",
    "            title = str(re.findall(r'html\">(.*?)</a>', atext)[0])\n",
    "            if title in titleList:\n",
    "                f.close()\n",
    "                continue\n",
    "            time = str(re.findall(r'<span class=\"bt-right\">(.*?)</span>',atext)[0])\n",
    "            dataRow.append(time)\n",
    "            dataRow.append(title)\n",
    "            dataRow.append(url)\n",
    "            dataRow.append(getCon(url))\n",
    "            csv_write.writerow(dataRow)\n",
    "            f.close()\n",
    "        \n",
    "getCsv(\"http://wjw.taian.gov.cn/col/col119732/index.html\")\n",
    "# for i in range(1,18):\n",
    "#     getCsv(\"http://wjw.taian.gov.cn/col/col119732/index.html?uid=286952&pageNum=\"+str(i))\n",
    "print(\"haha\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "http://www.jiangsu.gov.cn/art/2020/6/16/art_54678_9215247.html\n",
      "http://www.jiangsu.gov.cn/art/2020/5/29/art_54678_9190013.html\n",
      "http://www.jiangsu.gov.cn/art/2020/5/29/art_54678_9190000.html\n",
      "http://www.jiangsu.gov.cn/art/2020/5/28/art_54678_9189662.html\n",
      "http://www.jiangsu.gov.cn/art/2020/5/28/art_54678_9189671.html\n",
      "http://www.jiangsu.gov.cn/art/2020/5/28/art_54678_9189680.html\n",
      "http://www.jiangsu.gov.cn/art/2020/5/28/art_54678_9189587.html\n",
      "http://www.jiangsu.gov.cn/art/2020/5/28/art_54678_9189508.html\n",
      "http://www.jiangsu.gov.cn/art/2020/5/28/art_54678_9189517.html\n",
      "http://www.jiangsu.gov.cn/art/2020/5/28/art_54678_9189431.html  \n",
      "http://www.jiangsu.gov.cn/art/2020/5/28/art_54678_9189073.html\n",
      "http://www.jiangsu.gov.cn/art/2020/5/27/art_54678_9188305.html\n",
      "4\n",
      "7\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "# 泰州\n",
    "path = \"泰州1.csv\"\n",
    "\n",
    "def getCon(url):\n",
    "    con = \"\"\n",
    "    try:\n",
    "        html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "        soup = bs(html,'lxml')\n",
    "        td = soup.find('div',attrs={'id':\"zoom\"})\n",
    "        if td == None:\n",
    "            con += soup.get_text()\n",
    "        else:\n",
    "            con += td.get_text()\n",
    "    except Exception:\n",
    "        print(url)\n",
    "    return con\n",
    "    \n",
    "\n",
    "# headers = {'User-Agent':'Mozilla/5.0 (iPhone; CPU iPhone OS 7_1_2 like Mac OS X) App leWebKit/537.51.2 (KHTML, like Gecko) Version/7.0 Mobile/11D257 Safari/9537.53'}\n",
    "paperBase = \"http://wjw.taizhou.gov.cn\"\n",
    "paperTitle = []\n",
    "titleList = []\n",
    "def getCsv(url):\n",
    "    temp = urllib.request.Request(url,headers=headers)\n",
    "    html = urllib.request.urlopen(temp).read().decode('utf-8')\n",
    "    soup = bs(html,'lxml')\n",
    "    div = soup.find('div', attrs={'id':\"46605\"})\n",
    "    rs = str(div.find('script'))\n",
    "    date = re.findall(r'bt_time'+'\\''+'>'+'\\['+'(.*?)'+'\\]'+'</td>',rs)\n",
    "    arecord = re.findall(r'\"href=(.*?)</a></td>',rs) \n",
    "    for i in range(len(date)):\n",
    "        link = str(re.findall(r''+'\\''+'(.*?)'+'\\'',arecord[i])[0])\n",
    "        if \"http\" in link:\n",
    "            resLink = link\n",
    "        else:\n",
    "            resLink = paperBase+link\n",
    "        title = re.findall(r'title='+'\\''+'(.*?)'+'\\''+'>',arecord[i])\n",
    "        if title in titleList:\n",
    "            continue\n",
    "        titleList.append(title)\n",
    "        with open(path, 'a+') as f:\n",
    "            csv_write = csv.writer(f)\n",
    "            dataRow = []\n",
    "            dataRow.append(date[i])\n",
    "            dataRow.append(title[0])\n",
    "            dataRow.append(resLink)\n",
    "            dataRow.append(getCon(resLink))\n",
    "            csv_write.writerow(dataRow)\n",
    "            f.close()\n",
    "\n",
    "getCsv(\"http://wjw.taizhou.gov.cn/module/web/jpage/dataproxy.jsp?startrecord=856&endrecord=875&perpage=15\")\n",
    "# for i in range(1,12,3):\n",
    "#     print(i)\n",
    "#     getCsv(\"http://wjw.taizhou.gov.cn/col/col25808/index.html?uid=46605&pageNum=\"+str(i))\n",
    "\n",
    "# clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pa",
   "language": "python",
   "name": "crawler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
